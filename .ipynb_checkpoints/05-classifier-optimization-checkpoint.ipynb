{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4988daf4-346a-4a15-b8f8-4802ccdfb87b"
    }
   },
   "source": [
    "# Classifier Optimization\n",
    "\n",
    "\n",
    "In earlier notebooks, we explored classification and feature selection techniques. Throughout, we have emphasized the importance of using cross-validation to measure classifier performance and to perform feature selection. We introduced the [Pipeline package](http://scikit-learn.org/stable/modules/pipeline.html#pipeline) to help facilitate this cross-validation process. During the past exercises we didn't pay much attention to the parameters of the classifier and set them arbitrarily or based on intuition. Biased selection measures are problematic and can lead to circular inferences (aka [double dipping](http://www.nature.com/doifinder/10.1038/nn.2303)). In what follows we are going to investigate data-driven, unbiased techniques to optimize classification parameters such as choice of classifiers, cost parameters, and classification penalties. We will again use the Pipeline package to perform this optimization.\n",
    "\n",
    "We will be using the useful features from scikit-learn to perform cross-validation. scikit-learn also offers a simple procedure for building and automating the various steps involved in classifier optimization (e.g. data scaling => feature selection => parameter tuning). We will also explore these methods in this exercise.\n",
    "\n",
    "## Goal of this script\n",
    "1. Learn to detect and avoid circularity.\n",
    "2. Build a pipeline of steps to optimize classifier performance.    \n",
    "3. Use the pipeline to make an optimal classifier.  \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "[1. Load the data](#load-data)  \n",
    "\n",
    "[2. Circular Inference: How to avoid double dipping](#double_dipping)  \n",
    ">[2.1 Error: Voxel selection on all the data](#example-dd-vox-sel)  \n",
    ">[2.2 Test: Verify procedure on random (permuted) labels](#example-dd-random)\n",
    "  \n",
    "[3. Cross-validation: Hyper-parameter selection and regularization](#cross_val) \n",
    ">[3.1 Grid Search](#grid_search)  \n",
    ">[3.2 Regularization: L2 vs L1](#reg)  \n",
    ">[3.3 Nested Cross-validation: Hyper-parameter selection](#nested_cross_val)   \n",
    "\n",
    "\n",
    "[4. Make a pipeline](#pipeline)  \n",
    "\n",
    "Exercises\n",
    ">[Exercise 1](#ex1)  [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)   [7](#ex7)   \n",
    "\n",
    "[Novel contribution](#novel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** For this script we will use a localizer dataset from [Kim et al. (2017)](https://doi.org/10.1523/JNEUROSCI.3272-16.2017) again. Just to recap: The localizer consisted of 3 runs with 5 blocks of each category (faces, scenes and objects) per run. Each block was presented for 15s. Within a block, a stimulus was presented every 1.5s (1 TR). Between blocks, there was 15s (10 TRs) of fixation. Each run was 310 TRs. In the matlab stimulus file, the first row codes for the stimulus category for each trial (1 = Faces, 2 = Scenes, 3 = Objects). The 3rd row contains the time (in seconds, relative to the start of the run) when the stimulus was presented for each trial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1f7f9d75-833f-410f-8988-58c1618fa753"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# Import fMRI and general analysis libraries\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import sem\n",
    "from copy import deepcopy\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline \n",
    "sns.set(style = 'white', context='notebook', rc={\"lines.linewidth\": 2.5})\n",
    "sns.set(palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some helper functions\n",
    "from utils import load_labels, load_data, blockwise_sampling, label2TR, shift_timing, reshape_data\n",
    "from utils import normalize, decode\n",
    "# Load some constants\n",
    "from utils import vdc_data_dir, vdc_all_ROIs, vdc_label_dict, vdc_n_runs, vdc_hrf_lag, vdc_TR, vdc_TRs_run\n",
    "\n",
    "np.random.seed(0)\n",
    "print('Here are some constants specific to the VDC data:')\n",
    "print(f'data dir = {vdc_data_dir}')\n",
    "print(f'ROIs = {vdc_all_ROIs}')\n",
    "print(f'Labels = {vdc_label_dict}')\n",
    "print(f'number of runs = {vdc_n_runs}')\n",
    "print(f'1 TR = {vdc_TR} sec')\n",
    "print(f'HRF lag = {vdc_hrf_lag} sec')\n",
    "print(f'num TRs per run = {vdc_TRs_run}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data <a id=\"load-data\"></a>\n",
    "\n",
    "Load the data for one participant using the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = 8\n",
    "mask_name = 'PPA' # This is set in order to reduce memory demands in order to run within 4Gb, however, if you want to make this run on whole brain, then set this to ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the subject name\n",
    "sub = f'sub-{sub_id:02d}' \n",
    "\n",
    "# Convert the shift into TRs\n",
    "shift_size = int(vdc_hrf_lag / vdc_TR)  \n",
    "\n",
    "# Load subject labels\n",
    "stim_label_allruns = load_labels(vdc_data_dir, sub)\n",
    "\n",
    "# Load run_ids\n",
    "run_ids_raw = stim_label_allruns[5,:] - 1 \n",
    "\n",
    "# Load the fMRI data using a mask\n",
    "epi_mask_data_all = load_data(vdc_data_dir, sub, mask_name=mask_name)[0]\n",
    "\n",
    "# This can differ per participant\n",
    "print(f'{sub} data has {epi_mask_data_all.shape[1]} TRs and {epi_mask_data_all.shape[0]} voxels')\n",
    "TRs_run = int(epi_mask_data_all.shape[1] / vdc_n_runs)\n",
    "\n",
    "# Convert the timing into TR indexes\n",
    "stim_label_TR = label2TR(stim_label_allruns, vdc_n_runs, vdc_TR, TRs_run)\n",
    "\n",
    "# Shift the data some amount\n",
    "stim_label_TR_shifted = shift_timing(stim_label_TR, shift_size)\n",
    "\n",
    "# Perform the reshaping of the data\n",
    "bold_data_raw, labels_raw = reshape_data(stim_label_TR_shifted, epi_mask_data_all)\n",
    "\n",
    "# Normalize raw data within each run\n",
    "bold_normalized_raw = normalize(bold_data_raw, run_ids_raw)\n",
    "\n",
    "# Down sample the data to be blockwise rather than trialwise. \n",
    "# We'll use the blockwise data for all the following exercises\n",
    "bold_data, labels, run_ids = blockwise_sampling(bold_data_raw, labels_raw, run_ids_raw)\n",
    "\n",
    "# Normalize blockwise data within each run\n",
    "bold_normalized = normalize(bold_data, run_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Circular Inference: How to avoid double dipping <a id=\"double_dipping\"></a>\n",
    "\n",
    "The `GridSearchCV` method that you will learn about below makes it easy (though not guaranteed) to avoid double dipping. In previous exercises, we examined cases where double dipping is clear (e.g., training on all of the data and testing on a subset). However, double dipping can be subtler and harder to detect, such as in situations where you perform feature selection on the entire dataset before classification (as in last week's notebook).\n",
    "\n",
    "We now examine some cases of double dipping again. This is a critically important issue for doing fMRI analysis correctly and for obtaining generalizable results. We would like to emphasize through these examples:\n",
    "> 1. Whenever possible, _never_ look at your test data before building your model.\n",
    "> 2. If you *do* build your model using test data, verify your model on random noise. Your model should report chance level performance. If not, something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Common error: Voxel selection on all the data<a id=\"example-dd-vox-sel\"></a>\n",
    "\n",
    "Below we work through an exercise of a common type of double dipping: performing voxel selection on *all* of our data before splitting into training/testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sp = PredefinedSplit(run_ids)\n",
    "clf_score = []\n",
    "for train, test in sp.split():\n",
    "    \n",
    "    # Do voxel selection on all voxels\n",
    "    selected_voxels = SelectKBest(f_classif,k=100).fit(bold_normalized, labels)\n",
    "    \n",
    "    # Pull out the sample data\n",
    "    train_data = bold_normalized[train, :]\n",
    "    test_data = bold_normalized[test, :]\n",
    "\n",
    "    # Train and test the classifier\n",
    "    classifier = SVC(kernel=\"linear\", C=1)\n",
    "    clf = classifier.fit(selected_voxels.transform(train_data), labels[train])\n",
    "    score = clf.score(selected_voxels.transform(test_data), labels[test])\n",
    "    clf_score.append(score) \n",
    "\n",
    "print('Classification accuracy:', np.mean(clf_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test: Verify procedure on random (permuted) labels<a id=\"example-dd-random\"></a>\n",
    "\n",
    "One way to check if the procedure is valid is to test it on random data. We can do this by randomly assigning labels to every block. This breaks the true connection between the labels and the brain data, meaning that there should be **no** basis for reliable classification. So, if the classifier accuracy is above chance using permuted labels, we have done something wrong. Here, we apply our selection method and assess classifier accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_iters = 10  # How many permutations will we run? \n",
    "sp = PredefinedSplit(run_ids)\n",
    "clf_score = []\n",
    "\n",
    "for i in range(n_iters):\n",
    "    clf_score_i = []\n",
    "    \n",
    "    # Permute our labels to break the structure between brain data and labels\n",
    "    permuted_labels = np.random.permutation(labels)\n",
    "    \n",
    "    for train, test in sp.split():\n",
    "        # Do voxel selection on all voxels\n",
    "        selected_voxels = SelectKBest(f_classif,k=100).fit(bold_normalized, labels)\n",
    "\n",
    "        # Pull out the sample data\n",
    "        train_data = bold_normalized[train, :]\n",
    "        test_data = bold_normalized[test, :]\n",
    "\n",
    "        # Train and test the classifier\n",
    "        classifier = SVC(kernel=\"linear\", C=1)\n",
    "        clf = classifier.fit(selected_voxels.transform(bold_normalized), permuted_labels)\n",
    "        score = clf.score(selected_voxels.transform(test_data), permuted_labels[test])\n",
    "        clf_score_i.append(score)\n",
    "    clf_score.append(np.mean(clf_score_i))\n",
    "        \n",
    "print (f'Mean classification across {n_iters} folds: {np.mean(clf_score)}')\n",
    "print (f'Standard error: {sem(clf_score)}')\n",
    "print (f'Chance level: {np.round(1/3,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>We see above chance decoding accuracy! Something is wrong.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**<a id=\"ex1\"></a> Describe the double-dipping in the above code. Then, rewrite the code to fix the concerns and verify that accuracy on permuted test data is at chance level.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong> Sometimes you don't want to perform Leave-One-Run-Out </strong>\n",
    "<br>\n",
    "If we have different runs (or even a single run) but don't want to use them as the basis for your training/test splits (for instance, because we think that participants may respond differently on later vs. earlier runs; or we have only one run in the experiment in which we show a long movie), we can run into double dipping issues. For example, if you only have one run, it can still be useful to z-score each voxel (over time) within that run. Without z-scoring, voxels may have wildly different scales due to scanner drift or other confounds, distorting the classifier. Hence we need to normalize within run but this could be considered double dipping because each run includes both training and test data.  In these circumstances, it may (or may not) be fine to z-score over the entire dataset. <strong>Always verify the model performance by randomizing the labels!</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eee6bfd7-e6b6-4860-8501-6a3799dba268"
    }
   },
   "source": [
    "## 3. Cross-Validation: Hyperparameter selection and regularization <a id=\"cross_val\"></a>\n",
    "\n",
    "\n",
    "Each of the classifiers we have used so far has one or more \"hyperparameters\" used to configure and optimize the model based on the data and our goals. For instance, regularized logistic regression has a \"penalty\" hyperparameter which determines how much to emphasize the weight regularizing expression (e.g., L2 norm) when training the model. For an explanation on the distinction between hyperparameters and parameters, read [this Machine Learning Mastery Article](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/). \n",
    "\n",
    "**Exercise 2:** <a id=\"ex2\"></a> SVM has a \"cost\" ('C') hyperparameter, a.k.a. soft-margin hyperparameter. Look this up and briefly describe what it means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Grid Search <a id=\"grid_search\"></a>\n",
    "\n",
    "\n",
    "We want to pick the best cost hyperparameter for our dataset. To do this while avoiding double dipping, we can use cross-validation. Hyperparameters are often (but not always) continuous variables. Each hyperparameter can be considered as a dimension such that the set of hyperparameters is a space to be searched for effective values. The `GridSearchCV` method in [scikit-learn](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) explores the hyperparameter space by dividing it up into a grid of values to be searched exhaustively. \n",
    "\n",
    "To give you an intuition for how grid search works, imagine trying to figure out the climate you find most comfortable. Assume there are two (hyper)parameters that seem relevant -- temperature and humidity -- and a given climate can be defined by the combination of temperature and humidity values. A grid search would involve changing the value of each parameter with respect to the other in some fixed step size (e.g., 60 degrees and 50% humidity, 60 degrees and 60% humidity, 65 degrees and 60% humidity, etc.) and evaluating your preference for each combination.  \n",
    "\n",
    "Note that the number of steps and hyperparameters to search is up to you. But be aware of combinatorial explosion: the granularity of the search (the smaller the steps) and the number of hyperparameters considered increases the search time exponentially.\n",
    "\n",
    "`GridSearchCV` is an *extremely* useful tool for hyperparameter optimization because it is very flexible. You can look at different values of a hyperparameter, different [kernels](http://scikit-learn.org/stable/modules/svm.html), different training/test split sizes, etc. The input to the function is a dictionary where the key is the parameter of interest (the sides of the grid) and the values are the parameter increments to search over (the steps of the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to do a grid search over the SVM cost `C` hyperparameter (we call it a grid search now, even though only a single dimension is being searched over) and investigate the results. The output contains information about the best hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over different cost parameters (C)\n",
    "np.random.seed(0)\n",
    "parameters = {'C':[0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(\n",
    "    SVC(kernel='linear'),\n",
    "    parameters,\n",
    "    cv=PredefinedSplit(run_ids),\n",
    "    return_train_score=True\n",
    ")\n",
    "clf.fit(bold_normalized, labels)\n",
    "\n",
    "# What was the best classifier and cost?\n",
    "print(f'The best model: {clf.best_estimator_}')  \n",
    "# What was the best classification score?\n",
    "print(f'The score of the best model: {clf.best_score_}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to see more details from the cross-validation? All the results are stored in the dictionary `cv_results_`. Let's took a look at some of the important metrics stored here. For more details you can look at the `cv_results_` method on [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "You can print out `cv_results_` directly or for a nicer look you can import it into a [pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and print it out. Each row corresponds to one parameter combination.\n",
    "\n",
    "([Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) is a widely used data processing and machine learning package. Many of the functions work similarly to R, and it plays *very* nicely with Seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ugly way\n",
    "print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicer way (using pandas)\n",
    "results = pd.DataFrame(clf.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to do some different types of cross-validation hyperparameter tuning.\n",
    "\n",
    "**Exercise 3:**<a id=\"ex3\"></a> In machine learning, kernels are classes of algorithms that can be used to create a model. The (Gaussian) radial basis function (RBF) kernel is very common for SVM classifiers. Look up the RBF kernel and describe what it does. Also describe its parameter gamma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:**<a id=\"ex5\"></a> When would linear SVM be expected to outperform other kernels and why? Run an analysis in which you compare linear, polynomial, and RBF kernels for SVM using GridSearchCV. Print out the best parameters chosen by GridSearchCV using `clf.best_params_`.   \n",
    "\n",
    "*Hint*: You should not run 3 separate calls to GridSearchCV; you should treat these kernels as different hyperparameters to fit, along with `C = [10e-3, 10e0, 10e3]` and `gamma = [10e-3, 10e0, 10e3]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Regularization Example: L1 vs. L2 <a id=\"reg\"></a>\n",
    "\n",
    "Regularization is a technique that helps to reduce overfitting by assigning a penalty to the weights learned by the model. One common classifier used is [logistic regression](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc), which features different regularization options including the L1 and L2 penalties. An L1 (also called lasso) penalty penalizes the sum of the absolute values of the weights, whereas an L2 (also called ridge) penalty penalizes the sum of the squares of the weights. The L1 penalty leads to a sparser set of weights, with some high and the rest close to zero. The L2 penalty results in very small, nonzero weights. A more detailed explanation of (L2 and L1) regularization can be found [here](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms).\n",
    "\n",
    "Below, we compare the L1 and L2 penalties for logistic regression. For each of the penalty types, we run 3 folds and compute the correlation of weights across folds. If the weights on each voxel are similar across folds, then that can be thought of as a stable model. A higher correlation means a more stable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare L1 and L2 regularization in logistic regression\n",
    "np.random.seed(0)\n",
    "# Decode with L1 regularization\n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver = 'liblinear')\n",
    "model_l1, score_l1 = decode(bold_normalized, labels, run_ids, logreg_l1)\n",
    "print('Accuracy with L1 penalty: ', score_l1)\n",
    "\n",
    "# Decode with L2 regularization\n",
    "logreg_l2 = LogisticRegression(penalty='l2', solver = 'liblinear')\n",
    "model_l2, score_l2 = decode(bold_normalized, labels, run_ids, logreg_l2)\n",
    "print('Accuracy with L2 penalty: ', score_l2)\n",
    "print(f'StDev of scores with L1: {np.std(score_l1)} | StDev of scores with L2: {np.std(score_l2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can see the L1 penalty achieves a higher accuracy on 2/3 folds, but the standard deviation of those scores is 3x greater than with L2 - indicating its weights may be less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the weights for the 3 folds of the different types of regularization\n",
    "wts_l1 = np.stack([(model_l1[i].coef_).flatten() for i in range(len(model_l1))])\n",
    "wts_l2 = np.stack([(model_l2[i].coef_).flatten() for i in range(len(model_l2))])\n",
    "\n",
    "# Correlate the weights across each fold with the other folds\n",
    "corr_l1 = np.corrcoef(wts_l1)\n",
    "corr_l2 = np.corrcoef(wts_l2)\n",
    "\n",
    "# Plot the correlations across the folds\n",
    "fig,axes=plt.subplots(1,2,figsize=(10,4))\n",
    "plt.suptitle('Correlation of Weights Across Folds for L1 and L2 Regularization')\n",
    "\n",
    "axes[0].imshow(corr_l1, vmin=0, vmax=1)\n",
    "pos=[0,1,2]\n",
    "lab=['1','2','3']\n",
    "axes[0].set_title('L1, corr mean: %0.4f' % np.mean(corr_l1[np.triu(corr_l1, 1) > 0]))\n",
    "axes[0].set_xticks(pos,lab)\n",
    "axes[0].set_yticks(pos,lab)\n",
    "axes[0].set_xlabel('Fold')\n",
    "axes[0].set_ylabel('Fold')\n",
    "axes[0].grid(False)\n",
    "\n",
    "im=axes[1].imshow(corr_l2, vmin=0, vmax=1)\n",
    "pos=[0,1,2]\n",
    "lab=['1','2','3']\n",
    "axes[1].set_title('L2, corr mean: %0.4f' % np.mean(corr_l2[np.triu(corr_l2, 1) > 0]))\n",
    "axes[1].set_xticks(pos,lab)\n",
    "axes[1].set_yticks(pos,lab)\n",
    "axes[1].set_xlabel('Fold')\n",
    "axes[1].set_ylabel('Fold')\n",
    "axes[1].grid(False)\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**:<a id=\"ex5\"></a>  The L2 weights have a stronger correlation across folds. Why does L2 lead to more stable weights across folds?   \n",
    "*Hint*: Consider how L1 and L2 penalties would affect the weights assigned to two or more voxels that carry highly corrrelated information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "You might be tempted to run grid search CV, find the best result, and then report that result in your paper. The right way to handle this issue is to validate the model on unseen data, a procedure known as nested cross-validation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Nested cross-validation: Hyperparameter selection <a id='nested_cross_val'></a>   \n",
    "\n",
    "\n",
    "When we are writing a classification pipeline, nested cross-validation can be very useful. As the name suggests, this procedure nests a second cross-validation within folds of the first cross-validation. As before, we will divide data into training and test sets (outer loop), but additionally will divide the training set itself in order to set the hyperparameters into training and validation sets (inner loop).\n",
    "\n",
    "Thus, on each split we now have a training (inner), validation (inner), and test (outer) dataset; we will use leave-one-run-out for the validation set in the inner loop. Within the inner loop we train the model and find the optimal hyperparameters (i.e., that have the highest performance when tested on the validation data). The typical practice is to then retrain your model with these hyperparameters on both the training AND validation datasets and then evaluate on your held-out test dataset to get a score.\n",
    "\n",
    "![image](https://i.stack.imgur.com/vh1sZ.png)\n",
    "\n",
    "This is turtles all the way down -- you could have any number of inner loops. However, you will run into data issues quickly (not enough data for training) and you will also run the risk of over-fitting your data: you will find the optimal parameters for a small set of your data but this might not generalize to the rest of your data. For more description and a good summary of what you have learned so far then check [here](https://www.elderresearch.com/blog/nested-cross-validation-when-cross-validation-isnt-enough/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the training, validation, and testing set (the indexes that belong to each group)\n",
    "\n",
    "# Outer loop:\n",
    "# Split training (including validation) and testing set\n",
    "sp = PredefinedSplit(run_ids)\n",
    "for outer_idx, (train, test) in enumerate(sp.split()):\n",
    "    train_run_ids = run_ids[train]\n",
    "    print(f'Outer loop {outer_idx}:')\n",
    "    print(f'Testing: {test}')\n",
    "    \n",
    "    # Inner loop (implicit, in GridSearchCV):\n",
    "    # split training and validation set\n",
    "    sp_train = PredefinedSplit(train_run_ids)\n",
    "    for inner_idx, (train_inner, val) in enumerate(sp_train.split()):\n",
    "        print(f'\\tInner loop {inner_idx}:')\n",
    "        print(f'\\tTraining: {train[train_inner]}')\n",
    "        print(f'\\tValidation: {train[val]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of nested cross-validation using one subject and logistic regression\n",
    "\n",
    "# Outer loop:\n",
    "# Split training (including validation) and testing set\n",
    "sp = PredefinedSplit(run_ids)\n",
    "clf_score = []\n",
    "C_best = []\n",
    "for train, test in sp.split():\n",
    "    \n",
    "    # Pull out the sample data\n",
    "    train_run_ids = run_ids[train]\n",
    "    train_data = bold_normalized[train, :]\n",
    "    test_data = bold_normalized[test, :]\n",
    "    train_label = labels[train]\n",
    "    test_label = labels[test]\n",
    "    \n",
    "    # Inner loop (implicit, in GridSearchCV):\n",
    "    # Split training and validation set\n",
    "    sp_train = PredefinedSplit(train_run_ids)\n",
    "    \n",
    "    # Search over different regularization parameters: smaller values specify stronger regularization.\n",
    "    parameters = {'C':[10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e0, 10e1]}\n",
    "    inner_clf = GridSearchCV(\n",
    "        LogisticRegression(penalty='l2'),\n",
    "        parameters,\n",
    "        cv=sp_train,\n",
    "        return_train_score=True)\n",
    "    inner_clf.fit(train_data, train_label)\n",
    "    \n",
    "    # Find the best hyperparameter\n",
    "    C_best_i = inner_clf.best_params_['C']\n",
    "    C_best.append(C_best_i)\n",
    "    \n",
    "    # Train the classifier with the best hyperparameter using training and validation set\n",
    "    classifier = LogisticRegression(penalty='l2', C=C_best_i)\n",
    "    clf = classifier.fit(train_data, train_label)\n",
    "    \n",
    "    # Test the classifier\n",
    "    score = clf.score(test_data, test_label)\n",
    "    clf_score.append(score)\n",
    "    \n",
    "print ('Outer loop classification accuracy:', clf_score)\n",
    "print ('Best cost value:', C_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:**<a id=\"ex6\"></a> Set up a nested cross-validation loop for the PPA data of the first 3 subjects of the VDC data using SVM with a linear kernel. For each subject, use grid search to find the best hyperparameter C over the values [10e-2, 10e-1, 10e0, 10e1] in your inner loop. Report the best C for each run and the average and standard error of classification accuracies across folds, for each subject. \n",
    "\n",
    "Things to watch out for: \n",
    "- Be careful not to use hyperparameter optimization (e.g., with `GridSearchCV`) in both inner and outer loops of nested cross-validation\n",
    "- As always: If in doubt, check the [scikit-learn documentation](http://scikit-learn.org/stable/index.html) or [StackExchange Community](https://stackexchange.com/) for help\n",
    "- Running nested cross validation will take a couple of minutes. Grab a snack.\n",
    "- Use different variable names than the ones used above (such as `bold_normalized`, `labels` and `run_ids`) since we will still be using those data later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here\n",
    "\n",
    "# Create a function that will load and prepare the blockwise BOLD data, event labels, and run ids\n",
    "# for a single subject and ROI. \n",
    "# For inspiration, check out section 1: load data. You're going to use this function again later.\n",
    "# Write docstring-style description of your function (see week 2 notebook if you forget!).\n",
    "\n",
    "def load_single_subject_data(subject, ROI_name):\n",
    "    \n",
    "    \n",
    "    return bold_data, labels, run_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now code your nested cross-validation loops\n",
    "np.random.seed(0)\n",
    "subjects_exercise_6 = [1,2,3]\n",
    "\n",
    "# Save results for all subjects\n",
    "clf_accuracy_all_subjects, clf_Cs_all_subjects = [], []\n",
    "\n",
    "# Loop through subjects\n",
    "for sub in subjects_exercise_6:\n",
    "    # Load data, labels, and run_ids for this subject\n",
    "    \n",
    "    # Create outer loop: Split data & labels into training and testing sets\n",
    "\n",
    "        # Create another split to divide training data into training/validation for inner loop\n",
    "\n",
    "        # Use grid search over the parameters you want to optimize\n",
    "\n",
    "        # Grab the best parameter\n",
    "\n",
    "        # Train the classifier using all training data and the best hyperparameters\n",
    "\n",
    "        # Score the classifier on the test data and save\n",
    "    \n",
    "    # Save results for this subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the results\n",
    "print(f'Results for exercise 6')\n",
    "for s, subject in enumerate(subjects_exercise_6):\n",
    "    print(f'Sub-{subject:02d} C:{clf_Cs_all_subjects[s]}, accuracy (mean,std): {np.mean(clf_accuracy_all_subjects)}, {np.std(clf_accuracy_all_subjects)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Pipeline <a id=\"pipeline\"></a>\n",
    "\n",
    "In a previous notebook we had introduced the scikit-learn method, [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), that simplified running a sequence steps in an automated fashion. We will now use the pipeline to do feature selection and cross-validation. Below we create a pipeline with the following steps:   \n",
    ">Use PCA and choose the best option from a set of dimensions.  \n",
    ">Choose the best cost hyperparameter value for an SVM.\n",
    "\n",
    "It is then really easy to do cross-validation at different levels of this pipeline.\n",
    "\n",
    "The steps below are based on [this example in scikit-learn](https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#illustration-of-pipeline-and-gridsearchcv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "304fd1c3-80aa-4829-a36a-40c45dde562a"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the pipeline\n",
    "pipe = Pipeline([\n",
    "#         ('scale', preprocessing.StandardScaler()), # This could be part of our pipeline, but we already normalized our data.\n",
    "        ('reduce_dim', PCA()),\n",
    "        ('classify', SVC(kernel=\"linear\")),\n",
    "    ])\n",
    "\n",
    "# PCA dimensions\n",
    "component_steps = [10, 20, 30]\n",
    "\n",
    "# Classifier cost options\n",
    "c_steps = [10e-1, 10e0, 10e1, 10e2]\n",
    "\n",
    "# Build the grid search dictionary\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7)], \n",
    "        'reduce_dim__n_components': component_steps,\n",
    "        'classify__C': c_steps,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put it all together and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization parameter, will return to this later...\n",
    "n_jobs=1\n",
    "\n",
    "clf_pipe = GridSearchCV(pipe,\n",
    "                        cv=PredefinedSplit(run_ids),\n",
    "                        n_jobs=n_jobs,\n",
    "                        param_grid=param_grid,\n",
    "                        return_train_score=True\n",
    "                       )\n",
    "\n",
    "clf_pipe.fit(bold_normalized, labels)  # Run the pipeline\n",
    "\n",
    "print(clf_pipe.best_estimator_)  # What was the best classifier and parameters?\n",
    "print(clf_pipe.best_score_)  # What was the best classification score?\n",
    "\n",
    "# Sort results with declining mean test score\n",
    "cv_results = pd.DataFrame(clf_pipe.cv_results_)\n",
    "cv_results.sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:**<a id=\"ex7\"></a> Build a pipeline that performs the following steps:\n",
    "\n",
    "1. Voxel selection using the ANOVA method (`SelectKBest` and `f_classif`), optimizing the value `reduce_dim__k` with a grid search over `[10, 25, 50, 75, 100]`.\n",
    "2. Grid search over the linear and RBF SVM kernels.\n",
    "\n",
    "Run this pipeline for at least 5 subjects, in the FFA, and present your results as a bar graph with subjects on the x-axis and accuracy on the y-axis. Be sure to title this plot and add axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "np.random.seed(0)\n",
    "# Set up the pipeline.\n",
    "\n",
    "# Build the grid search dictionary.\n",
    "\n",
    "# Run the pipeline on each subject.\n",
    "\n",
    "# Plot a bar graph with subjects on the x-axis and accuracy on the y-axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:**<a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a> \n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 02/2018  \n",
    "T. Meissner minor edits  \n",
    "H. Zhang added random label and regularization exercises, change to PredefinedSplit, use normalized data, add solutions, other edits.  \n",
    "M. Kumar re-organized the sections and added section context.  \n",
    "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  \n",
    "C. Ellis incorporated comments from cmhn-s19   \n",
    "A.K. Sahoo updated code, links, and added hyperparameter values for better understanding.    \n",
    "M. Kumar updated notebook 01/2021.     \n",
    "T. Yates made edits for cmhn_s21.  \n",
    "E. Busch edits for cmhn_s22."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
